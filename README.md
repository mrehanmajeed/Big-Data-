# ğŸ“Š Big Data Web Log Analytics

A full-stack Big Data application that processes Apache web server logs using **Hadoop HDFS** and **Apache Spark**, stores the analytics in **MySQL**, and visualizes the insights via a **Django** backend and **React** frontend.

## ğŸ“ Project Structure

```
Apache/
â”‚
â”œâ”€â”€web-log-analytics/
â”‚      â”‚
â”‚      â”œâ”€â”€ ğŸ“„ Dockerfile                         # Docker build for Backend/Spark
â”‚      â”‚
â”‚      â”œâ”€â”€ ğŸ“‚ backend/                           # Django REST API
â”‚      â”‚   â”œâ”€â”€ ğŸ“„ manage.py                      # Django management script
â”‚      â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies
â”‚      â”‚   â”‚
â”‚      â”‚   â”œâ”€â”€ ğŸ“‚ analytics/                     # Main Django app
â”‚      â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py                  # Database models (3 models)
â”‚      â”‚   â”‚   â”œâ”€â”€ ğŸ“„ views.py                   # API viewsets (3 viewsets)
â”‚      â”‚   â”‚   â”œâ”€â”€ ğŸ“„ serializers.py             # DRF serializers
â”‚      â”‚   â”‚   â””â”€â”€ ğŸ“„ urls.py                    # API routing
â”‚      â”‚   â”‚
â”‚      â”‚   â””â”€â”€ ğŸ“‚ log_analytics/                 # Django project settings
â”‚      â”‚       â””â”€â”€ ğŸ“„ settings.py                # Project configuration
â”‚      â”‚
â”‚      â”œâ”€â”€ ğŸ“‚ frontend/                          # React application
â”‚      â”‚   â”œâ”€â”€ ğŸ“„ package.json                   # Node dependencies
â”‚      â”‚   â”œâ”€â”€ ğŸ“‚ src/
â”‚      â”‚   â”‚   â”œâ”€â”€ ğŸ“„ App.js                     # Main dashboard component
â”‚      â”‚   â”‚   â””â”€â”€ ğŸ“‚ services/
â”‚      â”‚   â”‚       â””â”€â”€ ğŸ“„ api.js                 # API client
â”‚      â”‚
â”‚      â””â”€â”€ ğŸ“‚ scripts/                           # Big Data Processing
â”‚          â”œâ”€â”€ ğŸ“„ process_logs_spark.py          # PySpark Log Analyzer
â”‚          â”œâ”€â”€ ğŸ“„ load_data_spark.py             # Spark Job Runner & DB Loader
â”‚          â””â”€â”€ ğŸ“„ requirements.txt               # Script dependencies
â”‚
â”œâ”€â”€ docker-compose.yml                           # Container Orchestration
â”œâ”€â”€ Apache_2k.log                                # Sample Log File 1
â””â”€â”€ Apache_access_sample.log                     # Sample Log File 2
```

## ğŸ”‘ Key Components

### Big Data Layer (Hadoop & Spark)
- **HDFS (Hadoop Distributed File System)**: Stores raw log files distributedly.
- **Apache Spark (PySpark)**: Processes logs in parallel using RDDs/DataFrames.
- **Spark Master/Worker**: Distributed computing cluster managed via Docker.

### Backend (Django)
- **REST API**: Serves analyzed data to the frontend.
- **MySQL Database**: Stores aggregated results (Downloads, Bandwidth, Traffic).
- **Dockerized**: Runs in a container connected to the Spark cluster.

### Frontend (React)
- **Dashboard**: Visualizes data using Chart.js (Bar, Pie, Line charts).
- **Real-time**: Fetches data from the Django API.

## ğŸ¯ Data Flow

```
Apache Log File
      â†“
[Hadoop HDFS] (Storage)
      â†“
[process_logs_spark.py] (PySpark Job)
      â†“
  Spark Cluster (Processing)
      â†“
[load_data_spark.py] (Driver)
      â†“
  MySQL Database
      â†“
[Django REST API]
      â†“
   JSON Response
      â†“
[React Frontend]
      â†“
Interactive Charts
```

## ğŸš€ How to Run

### 1. Start the Docker Cluster
```powershell
docker-compose up -d --build
```

### 2. Upload Logs to HDFS
Copy your log file into the Hadoop NameNode and put it into HDFS:
```powershell
docker cp Apache/Apache_2k.log namenode:/tmp/Apache_2k.log
docker-compose exec namenode hdfs dfs -mkdir -p /logs
docker-compose exec namenode hdfs dfs -put /tmp/Apache_2k.log /logs/Apache_2k.log
```

### 3. Run Spark Analysis
Trigger the Spark job to process the file and load data into MySQL:
```powershell
docker-compose run backend python scripts/load_data_spark.py hdfs://namenode:9000/logs/Apache_2k.log
```

### 4. View Dashboard
- **Backend API**: http://localhost:8000/api/
- **Frontend**: Run locally:
  ```powershell
  cd Apache/web-log-analytics/frontend
  npm start
  ```
  Access at: http://localhost:3000

## ğŸ“Š Database Schema

### FileDownload
- `file_path`: Unique path of the file.
- `download_count`: Number of times downloaded.
- `total_bytes`: Total data transferred.

### BandwidthUsage
- `ip_address`: Client IP.
- `total_bytes`: Total bandwidth consumed.
- `request_count`: Total requests made.

### TrafficHour
- `hour`: Hour of the day (0-23).
- `request_count`: Requests during this hour.
- `total_bytes`: Data transferred during this hour.

## ğŸ’» Technology Stack

- **Big Data**: Apache Hadoop 3.2, Apache Spark 3.5 (PySpark)
- **Backend**: Django 4.2, Django REST Framework, MySQL
- **Frontend**: React 18, Chart.js, Tailwind CSS
- **Infrastructure**: Docker, Docker Compose
